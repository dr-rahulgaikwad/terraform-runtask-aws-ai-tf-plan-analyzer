import json
import os
import re

import boto3
import botocore

from runtask_utils import generate_runtask_result
from tools.get_ami_releases import GetECSAmisReleases
from bedrock_utils import logger, stream_messages, tool_config
from utils.error_handling import retry_with_backoff
import xml.etree.ElementTree as ET

# Initialize model_id and region
model_id = os.environ.get("BEDROCK_LLM_MODEL")
guardrail_id = os.environ.get("BEDROCK_GUARDRAIL_ID", None)
guardrail_version = os.environ.get("BEDROCK_GUARDRAIL_VERSION", None)

# Config to avoid timeouts when using long prompts
config = botocore.config.Config(
    read_timeout=1800, connect_timeout=1800, retries={"max_attempts": 0}
)

session = boto3.Session()
bedrock_client = session.client(
    service_name="bedrock-runtime", config=config
)

# Input is the terraform plan JSON
def eval(tf_plan_json, tool_registry=None, structured_logger=None, metrics_emitter=None, output_formatter=None):

    #####################################################################
    ##### First, do generic evaluation of the Terraform plan output #####
    #####################################################################

    logger.info("##### Evaluating Terraform plan output #####")
    
    # Count resources for summary
    resource_changes = tf_plan_json.get("resource_changes", [])
    add_count = sum(1 for r in resource_changes if r.get("change", {}).get("actions") == ["create"])
    change_count = sum(1 for r in resource_changes if r.get("change", {}).get("actions") == ["update"])
    delete_count = sum(1 for r in resource_changes if r.get("change", {}).get("actions") == ["delete"])
    
    logger.info(f"Resource changes: {add_count} to add, {change_count} to change, {delete_count} to destroy")
    
    prompt = """
    You must respond with ONLY a JSON object. Do not include any explanatory text, conversation, or markdown formatting.

    Analyze the terraform plan and return this exact JSON structure:
    {
        "thinking": "brief analysis", 
        "resources": "## Plan-Summary\n\n**Networking**\n‚Ä¢ List VPCs, subnets, route tables with CIDR blocks\n‚Ä¢ Network ACLs and routing details\n\n**Security & Defaults**\n‚Ä¢ Security groups with ingress/egress rules (ports, protocols, CIDR)\n‚Ä¢ IAM roles and policies\n‚Ä¢ Encryption settings\n\n**Compute**\n‚Ä¢ EC2 instances (type, AMI ID, availability zone)\n‚Ä¢ Auto Scaling groups\n‚Ä¢ Launch templates/configurations\n\n**Storage**\n‚Ä¢ EBS volumes (size, type, encryption)\n‚Ä¢ S3 buckets (versioning, encryption, public access)\n\n**Tags**\n‚Ä¢ Common tags applied to resources", 
        "impact_analysis": "## üîç Impact Analysis\n\n### üö® Security Concerns\n- **Critical**: List any critical security issues (public S3, overly permissive SGs, unencrypted storage)\n- **High**: High-priority security concerns\n- **Medium**: Medium-priority security concerns\n- **Risk Level**: Overall security risk assessment\n\n### ‚ö†Ô∏è Configuration Issues\n- **Issue Type**: Configuration problems (missing tags, deprecated resources)\n- **Impact**: Consequence of these issues\n\n### üìä Operational Impact\n- **Infrastructure**: What's being deployed/changed/destroyed\n- **Availability**: Impact on high availability and fault tolerance\n- **Cost**: Estimated cost implications\n\n### üí° Recommendations\n- **Priority 1**: Most critical fixes needed\n- **Priority 2**: Secondary concerns to address\n- **Best Practices**: AWS best practice recommendations"
    }

    Terraform plan:
    """

    prompt += f"""
    {tf_plan_json["resource_changes"]}
    """

    messages = [
        {
            "role": "user",
            "content": [
                {
                    "text": prompt,
                }
            ],
        }
    ]

    system_text = "You are an assistant that helps reading infrastructure changes from JSON objects generated by terraform"

    # Wrap Bedrock API call with retry logic
    stop_reason, analysis_response = retry_with_backoff(
        lambda: stream_messages(bedrock_client, model_id, messages, system_text)
    )

    try:
        parsed_response = clean_response(analysis_response["content"][0]["text"])
        analysis_response_text = parsed_response["resources"]
        impact_analysis_text = parsed_response.get("impact_analysis", "No impact analysis available")
    except Exception as e:
        logger.error(f"Error parsing analysis response: {e}")
        analysis_response_text = "Error: Could not parse Terraform plan analysis"
        impact_analysis_text = "Error: Could not parse impact analysis"

    #####################################################################
    ######## Secondly, use tool orchestration for validation     ########
    #####################################################################
    logger.info("##### Running tool orchestration for infrastructure validation #####")
    
    # Get tool specifications from registry
    if tool_registry is not None:
        dynamic_tool_config = {
            "tools": tool_registry.to_bedrock_spec()
        }
        logger.info(f"Using tool registry with {len(dynamic_tool_config['tools'])} tools")
    else:
        dynamic_tool_config = tool_config
        logger.info("Using hardcoded tool configuration")
    
    # Single comprehensive prompt for AMI and infrastructure validation
    prompt = f"""
    Analyze this Terraform plan and provide a detailed AMI and infrastructure assessment.
    
    Use available tools to:
    1. **GetECSAmisReleases** - Get AMI release notes and version details for any EC2 AMI IDs found
    2. **EC2ValidatorTool** - Validate EC2 instance types and availability
    3. **S3ValidatorTool** - Check S3 bucket encryption and public access
    4. **SecurityGroupValidatorTool** - Identify overly permissive security group rules
    5. **CostEstimatorTool** - Estimate monthly costs
    
    Format output as:
    
    ## AMI-Summary
    
    **Current AMIs**
    ‚Ä¢ List current AMI IDs being used (if any)
    ‚Ä¢ AMI names and descriptions
    
    **New/Updated AMIs**
    ‚Ä¢ List new AMI IDs being deployed
    ‚Ä¢ AMI release notes and version information (use GetECSAmisReleases tool)
    ‚Ä¢ Changes from current to new AMI
    
    **Validation Results**
    ‚Ä¢ EC2 instance type validation results
    ‚Ä¢ Security group analysis (overly permissive rules)
    ‚Ä¢ S3 bucket security (encryption, public access)
    ‚Ä¢ Cost estimates (monthly)
    
    **Recommendations**
    ‚Ä¢ AMI update recommendations
    ‚Ä¢ Security improvements needed
    ‚Ä¢ Cost optimization suggestions
    
    Terraform plan: {tf_plan_json["resource_changes"]}
    """

    messages = [{"role": "user", "content": [{"text": prompt}]}]

    # Single Bedrock API call with tools
    stop_reason, response = retry_with_backoff(
        lambda: stream_messages(
            bedrock_client=bedrock_client,
            model_id=model_id,
            messages=messages,
            system_text="You are an AWS infrastructure analyst. Provide detailed AMI analysis with release notes, security validation, and cost estimates. Use GetECSAmisReleases tool for any AMI IDs found. Be specific with resource details.",
            tool_config=dynamic_tool_config,
        )
    )

    # Add response to message history
    messages.append(response)

    # Multi-turn function calling loop - handle tool execution requests
    max_iterations = 10  # Prevent infinite loops
    iteration = 0
    
    while stop_reason == "tool_use" and iteration < max_iterations:
        iteration += 1
        logger.info(f"Tool execution iteration {iteration}")
        
        for content in response["content"]:
            if "toolUse" in content:
                tool_use = content["toolUse"]
                tool_name = tool_use["name"]
                tool_input = tool_use["input"]
                tool_use_id = tool_use["toolUseId"]
                
                logger.info(f"Executing tool: {tool_name}")
                
                try:
                    # Dynamic tool routing using registry
                    if tool_registry is not None:
                        tool_instance = tool_registry.get_tool(tool_name)
                        
                        if tool_instance is None:
                            # Tool not found in registry
                            logger.error(f"Tool '{tool_name}' not found in registry")
                            tool_result = {
                                "toolUseId": tool_use_id,
                                "content": [{"json": {"error": f"Tool '{tool_name}' not found", "success": False}}],
                            }
                        else:
                            # Execute tool using registry
                            import time
                            start_time = time.time()
                            
                            tool_output = tool_instance.execute(tool_input)
                            
                            execution_time_ms = (time.time() - start_time) * 1000
                            
                            # Emit metrics if available
                            if metrics_emitter is not None:
                                metrics_emitter.emit_duration(f"ToolExecution_{tool_name}", execution_time_ms)
                                if tool_output.get("success", False):
                                    metrics_emitter.emit_count("ToolExecutionSuccess", 1)
                                else:
                                    metrics_emitter.emit_count("ToolExecutionFailure", 1)
                            
                            # Log tool execution if available
                            if structured_logger is not None:
                                structured_logger.log_tool_execution(
                                    tool_name=tool_name,
                                    success=tool_output.get("success", False),
                                    duration_ms=execution_time_ms
                                )
                            
                            tool_result = {
                                "toolUseId": tool_use_id,
                                "content": [{"json": tool_output}],
                            }
                    else:
                        # Fallback to hardcoded AMI tool for backward compatibility
                        if tool_name == "GetECSAmisReleases":
                            release_details = GetECSAmisReleases().execute(tool_input["image_ids"])
                            release_details_info = release_details if release_details else "No release notes were found the ami."
                            
                            tool_result = {
                                "toolUseId": tool_use_id,
                                "content": [{"json": {"release_detail": release_details_info}}],
                            }
                        else:
                            logger.error(f"Tool '{tool_name}' not supported in backward compatibility mode")
                            tool_result = {
                                "toolUseId": tool_use_id,
                                "content": [{"json": {"error": f"Tool '{tool_name}' not supported", "success": False}}],
                            }
                
                except Exception as e:
                    # Handle tool execution failure gracefully
                    logger.error(f"Tool {tool_name} failed: {type(e).__name__}: {e}", exc_info=True)
                    
                    # Emit failure metric if available
                    if metrics_emitter is not None:
                        metrics_emitter.emit_count("ToolExecutionFailure", 1)
                    
                    # Return error result to Bedrock so it can continue
                    error_message = f"Tool execution failed: {str(e)}"
                    tool_result = {
                        "toolUseId": tool_use_id,
                        "content": [{"json": {"error": error_message, "success": False}}],
                    }

                tool_result_message = {
                    "role": "user",
                    "content": [{"toolResult": tool_result}],
                }
                # Add the result info to message array
                messages.append(tool_result_message)

        # Send tool results back to model
        stop_reason, response = retry_with_backoff(
            lambda: stream_messages(
                bedrock_client=bedrock_client,
                model_id=model_id,
                messages=messages,
                system_text="Provide detailed infrastructure analysis with specific resource details. Use tools to validate configurations. Be concise but thorough.",
                tool_config=dynamic_tool_config,
            )
        )

        # Add response to message history
        messages.append(response)

    # Extract the actual response text from Bedrock
    if response and "content" in response and len(response["content"]) > 0:
        # Find text content in response
        result = ""
        for content_item in response["content"]:
            if "text" in content_item:
                result = content_item["text"]
                break
        
        if not result:
            result = "Error: No text response received from Bedrock"
            logger.error("No text content in Bedrock response")
    else:
        result = "Error: No response received from Bedrock"
        logger.error("No content received from Bedrock")

    logger.info("##### Analysis Complete #####")
    logger.info("Tool orchestration result: {}".format(result[:500]))

    results = []

    #####################################################################
    ##### Create structured multi-section output like original repo #####
    #####################################################################
    
    # Section 1: Plan-Summary (from analysis_response_text)
    guardrail_status_plan, guardrail_response_plan = guardrail_inspection(str(analysis_response_text))
    if guardrail_status_plan:
        results.append({
            "type": "task-result-outcomes",
            "attributes": {
                "outcome-id": "plan-summary",
                "description": "üìã Plan-Summary",
                "body": analysis_response_text[:9000],
                "tags": {
                    "status": [{"label": "Analyzed", "level": "info"}]
                }
            }
        })
    else:
        results.append({
            "type": "task-result-outcomes",
            "attributes": {
                "outcome-id": "plan-summary",
                "description": "üìã Plan-Summary",
                "body": "Output omitted due to guardrail: {}".format(guardrail_response_plan),
                "tags": {
                    "status": [{"label": "Blocked", "level": "warning"}]
                }
            }
        })
    
    # Section 2: Impact-Analysis (from impact_analysis_text)
    guardrail_status_impact, guardrail_response_impact = guardrail_inspection(str(impact_analysis_text))
    if guardrail_status_impact:
        results.append({
            "type": "task-result-outcomes",
            "attributes": {
                "outcome-id": "impact-analysis",
                "description": "üîç Impact-Analysis",
                "body": impact_analysis_text[:9000],
                "tags": {
                    "status": [{"label": "Analyzed", "level": "info"}]
                }
            }
        })
    else:
        results.append({
            "type": "task-result-outcomes",
            "attributes": {
                "outcome-id": "impact-analysis",
                "description": "üîç Impact-Analysis",
                "body": "Output omitted due to guardrail: {}".format(guardrail_response_impact),
                "tags": {
                    "status": [{"label": "Blocked", "level": "warning"}]
                }
            }
        })
    
    # Section 3: AMI-Summary (from tool orchestration result with AMI details)
    guardrail_status_ami, guardrail_response_ami = guardrail_inspection(str(result))
    if guardrail_status_ami:
        results.append({
            "type": "task-result-outcomes",
            "attributes": {
                "outcome-id": "ami-summary",
                "description": "üñ•Ô∏è AMI-Summary",
                "body": result[:9000],
                "tags": {
                    "status": [{"label": "Analyzed", "level": "info"}]
                }
            }
        })
    else:
        results.append({
            "type": "task-result-outcomes",
            "attributes": {
                "outcome-id": "ami-summary",
                "description": "üñ•Ô∏è AMI-Summary",
                "body": "Output omitted due to guardrail: {}".format(guardrail_response_ami),
                "tags": {
                    "status": [{"label": "Blocked", "level": "warning"}]
                }
            }
        })

    runtask_high_level = "ü§ñ AI-Powered Terraform Plan Analysis"
    return runtask_high_level, results

def guardrail_inspection(input_text, input_mode = 'OUTPUT'):

    #####################################################################
    ##### Inspect input / output against Bedrock Guardrail          #####
    #####################################################################

    if guardrail_id and guardrail_version:
        logger.info("##### Scanning Terraform plan output with Amazon Bedrock Guardrail #####")

        # Wrap Bedrock API call with retry logic
        response = retry_with_backoff(
            lambda: bedrock_client.apply_guardrail(
                guardrailIdentifier=guardrail_id,
                guardrailVersion=guardrail_version,
                source=input_mode,
                content=[
                    {
                        'text': {
                            'text': input_text,
                        }
                    },
                ]
            )
        )

        if response["action"] in ["GUARDRAIL_INTERVENED"]:
            logger.info("Guardrail action : {}".format(response["action"]))
            logger.info("Guardrail output : {}".format(response["outputs"]))
            
            # Log infrastructure-specific guardrail violations with full context
            assessments = response.get("assessments", [])
            for assessment in assessments:
                # Check for topic policy violations (infrastructure-specific)
                if "topicPolicy" in assessment:
                    topic_policy = assessment["topicPolicy"]
                    for topic in topic_policy.get("topics", []):
                        violation_type = topic.get("name", "Unknown")
                        action = topic.get("action", "Unknown")
                        
                        logger.warning(
                            "Infrastructure guardrail violation detected",
                            extra={
                                "violation_type": violation_type,
                                "action": action,
                                "input_mode": input_mode,
                                "recommendation": _get_guardrail_recommendation(violation_type)
                            }
                        )
                
                # Check for content policy violations
                if "contentPolicy" in assessment:
                    content_policy = assessment["contentPolicy"]
                    for filter_item in content_policy.get("filters", []):
                        filter_type = filter_item.get("type", "Unknown")
                        action = filter_item.get("action", "Unknown")
                        confidence = filter_item.get("confidence", "Unknown")
                        
                        logger.warning(
                            "Content policy violation detected",
                            extra={
                                "filter_type": filter_type,
                                "action": action,
                                "confidence": confidence,
                                "input_mode": input_mode
                            }
                        )
                
                # Check for sensitive information violations
                if "sensitiveInformationPolicy" in assessment:
                    sensitive_policy = assessment["sensitiveInformationPolicy"]
                    for pii_entity in sensitive_policy.get("piiEntities", []):
                        entity_type = pii_entity.get("type", "Unknown")
                        action = pii_entity.get("action", "Unknown")
                        
                        logger.warning(
                            "Sensitive information detected",
                            extra={
                                "entity_type": entity_type,
                                "action": action,
                                "input_mode": input_mode
                            }
                        )
            
            logger.debug("Guardrail assessments : {}".format(response["assessments"]))
            return False, response["outputs"][0]["text"]

        elif response["action"] in ["NONE"]:
            logger.info("No Guardrail action required")
            return True, "No Guardrail action required"

    else:
        return True, "Guardrail inspection skipped"


def _get_guardrail_recommendation(violation_type):
    """
    Get infrastructure-specific recommendations for guardrail violations.
    
    Args:
        violation_type: The type of guardrail violation (e.g., PublicS3Buckets)
    
    Returns:
        str: Recommendation for addressing the violation
    """
    recommendations = {
        "PublicS3Buckets": "Enable S3 Block Public Access settings and use bucket policies with specific principals instead of public access",
        "UnencryptedStorage": "Enable encryption at rest using AWS KMS or AES-256 encryption for all storage resources",
        "OverlyPermissiveIAM": "Follow the principle of least privilege by specifying exact actions and resources instead of using wildcards"
    }
    
    return recommendations.get(violation_type, "Review the resource configuration and apply AWS security best practices")

def clean_response(json_str):
    try:
        # First try to parse as-is
        return json.loads(json_str)
    except json.JSONDecodeError:
        try:
            # Remove any tags in the format <tag> or </tag>
            cleaned_str = re.sub(r'<\/?[\w\s]+>', '', json_str)

            # Find JSON content between braces
            start_brace = cleaned_str.find('{')
            last_brace = cleaned_str.rfind('}')

            if start_brace != -1 and last_brace != -1 and last_brace > start_brace:
                json_content = cleaned_str[start_brace:last_brace + 1]
                return json.loads(json_content)
            else:
                logger.error(f"No valid JSON braces found in response: {json_str}")
                return {"resources": "Error: No valid JSON structure found"}

        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error after cleaning: {e}, Original string: {json_str}")
            return {"resources": "Error: Could not parse response as JSON"}
    except Exception as e:
        logger.error(f"Unexpected error in clean_response: {e}, Original string: {json_str}")
        return {"resources": "Error: Unexpected parsing error"}
