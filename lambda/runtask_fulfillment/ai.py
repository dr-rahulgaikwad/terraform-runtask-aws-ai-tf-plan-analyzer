import json
import os
import re

import boto3
import botocore

from runtask_utils import generate_runtask_result
from tools.get_ami_releases import GetECSAmisReleases
from utils import logger, stream_messages, tool_config
from utils.error_handling import retry_with_backoff
import xml.etree.ElementTree as ET

# Initialize model_id and region
model_id = os.environ.get("BEDROCK_LLM_MODEL")
guardrail_id = os.environ.get("BEDROCK_GUARDRAIL_ID", None)
guardrail_version = os.environ.get("BEDROCK_GUARDRAIL_VERSION", None)

# Config to avoid timeouts when using long prompts
config = botocore.config.Config(
    read_timeout=1800, connect_timeout=1800, retries={"max_attempts": 0}
)

session = boto3.Session()
bedrock_client = session.client(
    service_name="bedrock-runtime", config=config
)

# Input is the terraform plan JSON
def eval(tf_plan_json, tool_registry=None, structured_logger=None, metrics_emitter=None):

    #####################################################################
    ##### First, do generic evaluation of the Terraform plan output #####
    #####################################################################

    logger.info("##### Evaluating Terraform plan output #####")
    prompt = """
    You must respond with ONLY a JSON object. Do not include any explanatory text, conversation, or markdown formatting.

    Analyze the terraform plan and return this exact JSON structure:
    {"thinking": "brief analysis", "resources": "list of resources being created, modified, or deleted", "impact_analysis": "assessment formatted as markdown with sections: ## üîç Impact Analysis\n\n### üö® Security Concerns\n- **Critical/High/Medium**: Description\n- **Risk Level**: Assessment\n\n### ‚ö†Ô∏è Configuration Issues\n- **Issue Type**: Description\n- **Impact**: Consequence\n\n### üìä Operational Impact\n- **Infrastructure**: What's being deployed\n- **Cost**: Cost implications\n\n### üí° Recommendations\n- **Priority 1**: Most critical fix\n- **Priority 2**: Secondary concerns\n- **Warning**: Important warnings"}

    Terraform plan:
    """

    prompt += f"""
    {tf_plan_json["resource_changes"]}
    """

    messages = [
        {
            "role": "user",
            "content": [
                {
                    "text": prompt,
                }
            ],
        }
    ]

    system_text = "You are an assistant that helps reading infrastructure changes from JSON objects generated by terraform"

    # Wrap Bedrock API call with retry logic
    stop_reason, analysis_response = retry_with_backoff(
        lambda: stream_messages(bedrock_client, model_id, messages, system_text)
    )

    try:
        parsed_response = clean_response(analysis_response["content"][0]["text"])
        analysis_response_text = parsed_response["resources"]
        impact_analysis_text = parsed_response.get("impact_analysis", "No impact analysis available")
    except Exception as e:
        logger.error(f"Error parsing analysis response: {e}")
        analysis_response_text = "Error: Could not parse Terraform plan analysis"
        impact_analysis_text = "Error: Could not parse impact analysis"

    #####################################################################
    ######## Secondly, use tool orchestration for validation     ########
    #####################################################################
    logger.info("##### Running tool orchestration for infrastructure validation #####")
    
    # Get tool specifications from registry (or fall back to hardcoded config)
    if tool_registry is not None:
        dynamic_tool_config = {
            "tools": tool_registry.to_bedrock_spec()
        }
        logger.info(f"Using dynamic tool registry with {len(dynamic_tool_config['tools'])} tools")
    else:
        # Fallback to hardcoded tool_config for backward compatibility
        dynamic_tool_config = tool_config
        logger.info("Using hardcoded tool configuration (backward compatibility mode)")
    
    prompt = f"""
    Analyze the Terraform plan and use available tools to validate infrastructure configurations.
    
    For EC2 instances: Check instance type availability and AMI release notes
    For S3 buckets: Validate public access and encryption settings
    For Security Groups: Check for overly permissive rules
    For cost analysis: Estimate monthly costs for compute resources
    
    Terraform plan: {tf_plan_json["resource_changes"]}
    
    Previous analysis: {analysis_response_text}
    """

    messages = [{"role": "user", "content": [{"text": prompt}]}]

    # Wrap Bedrock API call with retry logic
    stop_reason, response = retry_with_backoff(
        lambda: stream_messages(
            bedrock_client=bedrock_client,
            model_id=model_id,
            messages=messages,
            system_text="Provide direct, technical analysis of infrastructure changes. Use available tools to validate configurations.",
            tool_config=dynamic_tool_config,
        )
    )

    # Add response to message history
    messages.append(response)

    # Multi-turn function calling loop - handle tool execution requests
    max_iterations = 10  # Prevent infinite loops
    iteration = 0
    
    while stop_reason == "tool_use" and iteration < max_iterations:
        iteration += 1
        logger.info(f"Tool execution iteration {iteration}")
        
        for content in response["content"]:
            if "toolUse" in content:
                tool_use = content["toolUse"]
                tool_name = tool_use["name"]
                tool_input = tool_use["input"]
                tool_use_id = tool_use["toolUseId"]
                
                logger.info(f"Executing tool: {tool_name}")
                
                try:
                    # Dynamic tool routing using registry
                    if tool_registry is not None:
                        tool_instance = tool_registry.get_tool(tool_name)
                        
                        if tool_instance is None:
                            # Tool not found in registry
                            logger.error(f"Tool '{tool_name}' not found in registry")
                            tool_result = {
                                "toolUseId": tool_use_id,
                                "content": [{"json": {"error": f"Tool '{tool_name}' not found", "success": False}}],
                            }
                        else:
                            # Execute tool using registry
                            import time
                            start_time = time.time()
                            
                            tool_output = tool_instance.execute(tool_input)
                            
                            execution_time_ms = (time.time() - start_time) * 1000
                            
                            # Emit metrics if available
                            if metrics_emitter is not None:
                                metrics_emitter.emit_duration(f"ToolExecution_{tool_name}", execution_time_ms)
                                if tool_output.get("success", False):
                                    metrics_emitter.emit_count("ToolExecutionSuccess", 1)
                                else:
                                    metrics_emitter.emit_count("ToolExecutionFailure", 1)
                            
                            # Log tool execution if available
                            if structured_logger is not None:
                                structured_logger.log_tool_execution(
                                    tool_name=tool_name,
                                    success=tool_output.get("success", False),
                                    duration_ms=execution_time_ms
                                )
                            
                            tool_result = {
                                "toolUseId": tool_use_id,
                                "content": [{"json": tool_output}],
                            }
                    else:
                        # Fallback to hardcoded AMI tool for backward compatibility
                        if tool_name == "GetECSAmisReleases":
                            release_details = GetECSAmisReleases().execute(tool_input["image_ids"])
                            release_details_info = release_details if release_details else "No release notes were found the ami."
                            
                            tool_result = {
                                "toolUseId": tool_use_id,
                                "content": [{"json": {"release_detail": release_details_info}}],
                            }
                        else:
                            logger.error(f"Tool '{tool_name}' not supported in backward compatibility mode")
                            tool_result = {
                                "toolUseId": tool_use_id,
                                "content": [{"json": {"error": f"Tool '{tool_name}' not supported", "success": False}}],
                            }
                
                except Exception as e:
                    # Handle tool execution failure gracefully
                    logger.error(f"Tool {tool_name} failed: {type(e).__name__}: {e}", exc_info=True)
                    
                    # Emit failure metric if available
                    if metrics_emitter is not None:
                        metrics_emitter.emit_count("ToolExecutionFailure", 1)
                    
                    # Return error result to Bedrock so it can continue
                    error_message = f"Tool execution failed: {str(e)}"
                    tool_result = {
                        "toolUseId": tool_use_id,
                        "content": [{"json": {"error": error_message, "success": False}}],
                    }

                tool_result_message = {
                    "role": "user",
                    "content": [{"toolResult": tool_result}],
                }
                # Add the result info to message array
                messages.append(tool_result_message)

        # Send the messages, including the tool result, to the model.
        # Wrap Bedrock API call with retry logic
        stop_reason, response = retry_with_backoff(
            lambda: stream_messages(
                bedrock_client=bedrock_client,
                model_id=model_id,
                messages=messages,
                system_text="Provide direct, technical analysis of infrastructure changes. Use available tools to validate configurations.",
                tool_config=dynamic_tool_config,
            )
        )

        # Add response to message history
        messages.append(response)

    # Extract the actual response text from Bedrock
    if response and "content" in response and len(response["content"]) > 0:
        # Find text content in response
        result = ""
        for content_item in response["content"]:
            if "text" in content_item:
                result = content_item["text"]
                break
        
        if not result:
            result = "Error: No text response received from Bedrock"
            logger.error("No text content in Bedrock response")
    else:
        result = "Error: No response received from Bedrock"
        logger.error("No content received from Bedrock")

    #####################################################################
    ######### Third, generate short summary                     #########
    #####################################################################

    logger.info("##### Generating short summary #####")
    prompt = f"""
    Provide a concise summary of these Terraform changes. Focus on what resources are being created, modified, or deleted:

    {tf_plan_json["resource_changes"]}
    """
    message_desc = [{"role": "user", "content": [{"text": prompt}]}]
    # Wrap Bedrock API call with retry logic
    stop_reason, response = retry_with_backoff(
        lambda: stream_messages(
            bedrock_client=bedrock_client,
            model_id=model_id,
            messages=message_desc,
            system_text="Provide a direct, technical summary without conversational language.",
            tool_config=None,
        )
    )

    # Extract the actual response text from Bedrock
    if response and "content" in response and len(response["content"]) > 0:
        description = response["content"][0]["text"]
    else:
        description = "Error: No response received from Bedrock"
        logger.error("No response content received from Bedrock")

    logger.info("##### Report #####")
    logger.info("Analysis : {}".format(analysis_response_text))
    logger.info("Impact Analysis: {}".format(impact_analysis_text))
    logger.info("Tool orchestration summary: {}".format(result))
    logger.info("Terraform plan summary: {}".format(description))

    results = []

    guardrail_status, guardrail_response = guardrail_inspection(str(description))
    if guardrail_status:
        results.append(generate_runtask_result(outcome_id="Plan-Summary", description="Summary of Terraform plan", result=description[:9000])) # body max limit of 10,000 chars
    else:
        results.append(generate_runtask_result(outcome_id="Plan-Summary", description="Summary of Terraform plan", result="Output omitted due to : {}".format(guardrail_response)))
        description = "Bedrock guardrail triggered : {}".format(guardrail_response)

    guardrail_status, guardrail_response = guardrail_inspection(str(impact_analysis_text))
    if guardrail_status:
        results.append(generate_runtask_result(outcome_id="Impact-Analysis", description="Security and operational impact assessment", result=impact_analysis_text[:9000]))
    else:
        results.append(generate_runtask_result(outcome_id="Impact-Analysis", description="Security and operational impact assessment", result="Output omitted due to : {}".format(guardrail_response)))

    guardrail_status, guardrail_response = guardrail_inspection(str(result))
    if guardrail_status:
        results.append(generate_runtask_result(outcome_id="Validation-Summary", description="Infrastructure validation and tool analysis", result=result[:9000]))
    else:
        results.append(generate_runtask_result(outcome_id="Validation-Summary", description="Infrastructure validation and tool analysis", result="Output omitted due to : {}".format(guardrail_response)))

    runtask_high_level ="Terraform plan analyzer using Amazon Bedrock, expand the findings below to learn more. Click `view more details` to get the detailed logs"
    return runtask_high_level, results

def guardrail_inspection(input_text, input_mode = 'OUTPUT'):

    #####################################################################
    ##### Inspect input / output against Bedrock Guardrail          #####
    #####################################################################

    if guardrail_id and guardrail_version:
        logger.info("##### Scanning Terraform plan output with Amazon Bedrock Guardrail #####")

        # Wrap Bedrock API call with retry logic
        response = retry_with_backoff(
            lambda: bedrock_client.apply_guardrail(
                guardrailIdentifier=guardrail_id,
                guardrailVersion=guardrail_version,
                source=input_mode,
                content=[
                    {
                        'text': {
                            'text': input_text,
                        }
                    },
                ]
            )
        )

        if response["action"] in ["GUARDRAIL_INTERVENED"]:
            logger.info("Guardrail action : {}".format(response["action"]))
            logger.info("Guardrail output : {}".format(response["outputs"]))
            
            # Log infrastructure-specific guardrail violations with full context
            assessments = response.get("assessments", [])
            for assessment in assessments:
                # Check for topic policy violations (infrastructure-specific)
                if "topicPolicy" in assessment:
                    topic_policy = assessment["topicPolicy"]
                    for topic in topic_policy.get("topics", []):
                        violation_type = topic.get("name", "Unknown")
                        action = topic.get("action", "Unknown")
                        
                        logger.warning(
                            "Infrastructure guardrail violation detected",
                            extra={
                                "violation_type": violation_type,
                                "action": action,
                                "input_mode": input_mode,
                                "recommendation": _get_guardrail_recommendation(violation_type)
                            }
                        )
                
                # Check for content policy violations
                if "contentPolicy" in assessment:
                    content_policy = assessment["contentPolicy"]
                    for filter_item in content_policy.get("filters", []):
                        filter_type = filter_item.get("type", "Unknown")
                        action = filter_item.get("action", "Unknown")
                        confidence = filter_item.get("confidence", "Unknown")
                        
                        logger.warning(
                            "Content policy violation detected",
                            extra={
                                "filter_type": filter_type,
                                "action": action,
                                "confidence": confidence,
                                "input_mode": input_mode
                            }
                        )
                
                # Check for sensitive information violations
                if "sensitiveInformationPolicy" in assessment:
                    sensitive_policy = assessment["sensitiveInformationPolicy"]
                    for pii_entity in sensitive_policy.get("piiEntities", []):
                        entity_type = pii_entity.get("type", "Unknown")
                        action = pii_entity.get("action", "Unknown")
                        
                        logger.warning(
                            "Sensitive information detected",
                            extra={
                                "entity_type": entity_type,
                                "action": action,
                                "input_mode": input_mode
                            }
                        )
            
            logger.debug("Guardrail assessments : {}".format(response["assessments"]))
            return False, response["outputs"][0]["text"]

        elif response["action"] in ["NONE"]:
            logger.info("No Guardrail action required")
            return True, "No Guardrail action required"

    else:
        return True, "Guardrail inspection skipped"


def _get_guardrail_recommendation(violation_type):
    """
    Get infrastructure-specific recommendations for guardrail violations.
    
    Args:
        violation_type: The type of guardrail violation (e.g., PublicS3Buckets)
    
    Returns:
        str: Recommendation for addressing the violation
    """
    recommendations = {
        "PublicS3Buckets": "Enable S3 Block Public Access settings and use bucket policies with specific principals instead of public access",
        "UnencryptedStorage": "Enable encryption at rest using AWS KMS or AES-256 encryption for all storage resources",
        "OverlyPermissiveIAM": "Follow the principle of least privilege by specifying exact actions and resources instead of using wildcards"
    }
    
    return recommendations.get(violation_type, "Review the resource configuration and apply AWS security best practices")

def clean_response(json_str):
    try:
        # First try to parse as-is
        return json.loads(json_str)
    except json.JSONDecodeError:
        try:
            # Remove any tags in the format <tag> or </tag>
            cleaned_str = re.sub(r'<\/?[\w\s]+>', '', json_str)

            # Find JSON content between braces
            start_brace = cleaned_str.find('{')
            last_brace = cleaned_str.rfind('}')

            if start_brace != -1 and last_brace != -1 and last_brace > start_brace:
                json_content = cleaned_str[start_brace:last_brace + 1]
                return json.loads(json_content)
            else:
                logger.error(f"No valid JSON braces found in response: {json_str}")
                return {"resources": "Error: No valid JSON structure found"}

        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error after cleaning: {e}, Original string: {json_str}")
            return {"resources": "Error: Could not parse response as JSON"}
    except Exception as e:
        logger.error(f"Unexpected error in clean_response: {e}, Original string: {json_str}")
        return {"resources": "Error: Unexpected parsing error"}
